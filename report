The Naive bayes classifier and the bidirectional LSTM have proved to be the most effective of the models used thus far. I had also tested GRUs and unidirectional LSTMs, but the bidirectional LSTM outperformed them both (although it outperformed the former by not much of a margin). The bidirectional LSTM consists of a singular bidirectional lstm layer with a 100 hidden units, an embedding size that is the size of the vocabulary, and an embedding dimension of 20, followed by multiple dense layers without dropouts to avoid the problem of overfitting, as the model tends to overfit on the training data very quickly, otherwise. Added regularization didnâ€™t make much of a difference after adding the dropout layer, and so it was dropped from the model. Since the vocabulary size is large, it may not have enough data to work as effectively as we would hope for it to. For instance, a lot of the threads tagged as political tend to include proper nouns and other such words that imply that the post is related to politics, but it is likely that the embedding may not pick up on this. Pre-trained word embeddings did not prove to be very effective for similar reasons. In my opinion, the best way to go about this task would have been to include the body of the text as well, as it generally seems to contain more information that the model would be able to pick up on. However, for certain posts, especially ones tagged under AskIndia and such, the body is extremely large. An alternative in such cases would be to extract the relevant parts of this text, rather than processing it as a whole. 

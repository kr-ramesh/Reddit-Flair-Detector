{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreprocessingReddit.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVsk38ahjzj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "x=files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-4u-3vrkEEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('DV (1).csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpPAhEtGFU61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "def preprocess(text):    \n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  #word_tokens = word_tokenize(text)\n",
        "  word_tokens= re.split(' |,|-|\\|*/|',text)\n",
        "  txt= [w for w in word_tokens if not w in stop_words]\n",
        "  ls=[]\n",
        "  for w in txt:\n",
        "    r=re.sub(\"'s\",'',w)\n",
        "    r=re.sub('[^a-zA-Z]+', '', r)\n",
        "    if(len(r)>2):\n",
        "      ls.append(r)\n",
        "  ls=[lemmatizer.lemmatize(w) for w in ls if len(w)<10] \n",
        "  ls=' '.join(ls)\n",
        "  ls=re.sub(' +', ' ', ls)\n",
        "  ls=ls.lower()\n",
        "  print(ls)\n",
        "  return ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moA47s_6kGmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flairs=[]\n",
        "text=[]\n",
        "for i in df.index:\n",
        "  x=preprocess(df['Title'][i])\n",
        "  length=len(x.split())\n",
        "  if(length>=5 and length<=23):\n",
        "    flairs.append(df['Flair'][i])\n",
        "    text.append(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPgy4hWUmpuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(flairs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHi8XCYpkmxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "a = pd.DataFrame(\n",
        "    {'Flair': flairs,\n",
        "     'Text': text\n",
        "    })\n",
        "a.to_csv('RedditPPD.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJSA6b9Rkw-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('RedditPPD.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
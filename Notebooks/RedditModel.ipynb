{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "RedditModel.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "020JtiqocmVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pYcZSTNbrtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "embedding_dim = 20\n",
        "tr = .99\n",
        "counts=[0,0,0,0,0,0,0,0,0]\n",
        "def convert_to_int(word):\n",
        "    word_dict = {'Coronavirus':0, 'AskIndia':1, 'Non-Political':2, 'Policy/Economy':3, 'Business/Finance':4, 'Science/Technology':5, 'Sports':6, 'Politics':7,\n",
        "                'Food':8}\n",
        "    return word_dict[word]\n",
        "flair_list=['Coronavirus','AskIndia','Non-Political','Policy/Economy','Business/Finance','Science/Technology','Sports','Politics','Food']\n",
        "labels=[]\n",
        "sentences=[]\n",
        "with open(\"main_new.csv\") as csvfile:\n",
        "  reader=csv.reader(csvfile,delimiter=',')\n",
        "  next(reader)\n",
        "  for row in reader:\n",
        "      if((row[1] in flair_list) and counts[flair_list.index(row[1])]<2000):\n",
        "          counts[flair_list.index(row[1])]+=1\n",
        "          labels.append(convert_to_int(row[1]))\n",
        "          sentences.append(row[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eq-NHrIoKec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXzR69ZhXCcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "# autosave best Model\n",
        "best_model = tf.keras.callbacks.ModelCheckpoint('mymodel.h5',monitor='val_acc',verbose=1,save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY6poUpKeUkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=np.array(labels)\n",
        "(unique, counts) = np.unique(x, return_counts=True)\n",
        "print(unique)\n",
        "print(counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Haofr19brtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "c = list(zip(sentences, labels))\n",
        "\n",
        "random.shuffle(c)\n",
        "\n",
        "sentences, labels = zip(*c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeG6wIY9brtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer=Tokenizer(oov_token='<00V>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index=tokenizer.word_index\n",
        "\n",
        "seq=tokenizer.texts_to_sequences(sentences)\n",
        "seq=pad_sequences(seq,padding='post')\n",
        "\n",
        "\n",
        "training_size=int(len(sentences)*tr)\n",
        "trainset=seq[:training_size]\n",
        "trainlabels=labels[:training_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Oxg4ILMbrtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testset=seq[training_size:]\n",
        "testlabels=labels[training_size:]\n",
        "\n",
        "testlab=np.array((testlabels))\n",
        "trainlab=np.array((trainlabels))\n",
        "print(trainlab.shape)\n",
        "print(trainset.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dns76WoHbrtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=len(word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN0Rh4Zbrtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import metrics\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size+1, embedding_dim),\n",
        "    tf.keras.layers.Bidirectional(LSTM(100,return_sequences=False)),\n",
        "    #tf.keras.layers.LSTM(64, return_sequences=False),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    #tf.keras.layers.Dense(256,activation='relu'),\n",
        "    #tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(128,activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(128,activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(32,activation='relu'),\n",
        "    tf.keras.layers.Dense(9,activation='softmax')\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0EFUAXubrtf",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "history = model.fit(trainset, trainlab, epochs=10, validation_data=(testset, testlab), verbose=2,callbacks=[best_model])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rahDSPI1brti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}